---
title: "Central Limit Theorem"
layout: post
---

The central limit theorem states that in many situations, when a bunch of independent random variables are added together, their normalized sum tends to a Gaussian distribution.

It is easy to see why this theorem must be true. For simplicity, we could consider a situation of rolling a dice n times. When the dice is rolled once, there are six possibilities for the outcome and each of it is equally likely. Now we could roll the dice n times and ask for example, what is the likelihood for the sum of the outcomes to be 6n. Of course, there is only one way the sum could be 6n and that is when each of the n throws turn out an outcome of 6. Assuming the dice is fair, the probability of this to happen is low and is in fact just 6<sup>-n</sup>. By a similar argument, the probability for the sum to be exactly n is also 6<sup>-n</sup>. On the other hand, what is the probability for the sum to be 3n? There are many ways one could get a sum of 3n. One possibility is when each of the throws give an outcome of 3. Or the first five being 1,2,3,4,5 and the rest just 3. Now one could also permute the order in which the outcomes 1,2,3,4,5 occur and you would still get a sum of 3n. It is clear that there are a numerous possibilities to get a sum of 3n and hence a higher probability. Similarly, we could consider other values for the sum of the outcomes. As one moves from n to 3n to 6n, the probability for the sum starts at the lowest possibile value, increases and then again decreases to the lowest. At the very least, this smells like a Gaussian distribution. In fact, it can be rigorously proven that in the large n limit, the final distribution is indeed a Gaussian. By normalizing the sum i.e. by taking the mean, we would still get a Gaussian distribution.

In this blog, I have tried to demonstrate how starting with different probability distributions one gets a Gaussian-like distribution by taking a mean of n samples of the original distribution. The "Gaussian-like" becomes a true Gaussian in the large n limit.

Let us start by importing the neccessary libraries.

{% highlight python %}
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

import numpy as np
import matplotlib.pyplot as plt
{% endhighlight %}

The following function takes in a distribution object as argument, samples from it, finds the mean of every $n$ samples, and returns them.

{% highlight python %}
def mean_of_random_variables(distribution, n=10):
    sample_size = 1000*n
    x = distribution.sample(sample_size)
    z = []
    for i in range(int(sample_size/n)):
        temp = 0
        for j in range(n):
            temp += x[n*i+j]
        z.append(temp/n)
    return z
{% endhighlight %}    

Now let us sample from various distributions, find the mean and plot them in histograms to see the Gaussian distribution emerge.

##Uniform distribution

Let us start with the simplest of probability distributions: the uniform distribution. First, we create a uniform distribution object which is then passed to mean_of_random_variables() function. The function returns an array of values obtained by taking the mean of n samples of the uniform distribution.

{% highlight python %}
uniform = tfd.Uniform(low=0.0, high=1)
{% endhighlight %}


{% highlight python %}

z = mean_of_random_variables(uniform, n=50)
plt.hist(z, bins = 50)
plt.show()
{% endhighlight %}
